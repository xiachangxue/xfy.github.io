@article{LI2024123939,
title = {RDTN: Residual Densely Transformer Network for hyperspectral image classification},
journal = {Expert Systems with Applications},
volume = {250},
pages = {123939},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123939},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424008054},
author = {Yan Li and Xiaofei Yang and Dong Tang and Zheng Zhou},
keywords = {Hyperspectral image classification, Transformers, Convolution neural network},
abstract = {Transformer-based methods have achieved significant success in hyperspectral image (HSI) classification, which attribute to the strong capability of capturing the global dependencies from the input. However, the existing Transformer-based HSI classification methods are challenged in retrieving sufficient abound local information using the linear projection modules. Moreover, they do not fully use the hierarchical representations extracted from the original hyperspectral images. To overcome these challenges, this paper proposes a novel transformer model, called Residual Densely Transformer Network (RDTN) to comprehensively exploit the multi-hierarchical features and the local–global dependencies along the spatial–spectral dimensions from the hyperspectral images. Specially, the proposed RDTN is built with two modules: a Cross-Scale Convolution Attention (CSCA) module to extract abundant local spatial–spectral features using the multiscale convolution attention layers, and a Local Residual Transformer Block (LRTB) to respectively capture the abundant global dependencies along the spatial–spectral dimensions. Additionally, LRTB uses a residual connection operation to make full use of the hierarchical representations of all the transformer encoder layers. After acquiring dense global representations, we introduce a Global Residual Connection (GRC) to jointly fuse the local features obtained by CSCA, and then feed the fusion representations into the final avg-pooling layer and a classifier to predict the category. Finally, we conduct the extensive experiments based on four public benchmarks datasets, whose results are demonstrated that the proposed RDTN outperforms the state-of-the-art methods. The codes of this work are available at https://github.com/xiachangxue/DeepHyperX.}
}